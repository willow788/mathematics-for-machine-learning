# ðŸ¤“ Deep Dive: Mathematical Details

## Multivariate Calculus

### Partial Derivatives

Computed via the definition:
\[
\frac{\partial f}{\partial x}(x, y) \approx \frac{f(x + h, y) - f(x - h, y)}{2h}
\]
Numerical methods implemented: forward, backward, central difference.

### Gradient

The vector of all partial derivatives. Used for optimization direction (`gradient descent`).

### Hessian Matrix

Second partial derivatives form the Hessian matrix, describing the curvature and local behavior of a function.
\[
H_{ij} = \frac{\partial^2 f}{\partial x_i \partial x_j}
\]

### Lagrange Multipliers

Technique for constrained optimizationâ€”find optima of \( f(x, y) \) subject to \( g(x, y) = c \) by solving:
\[
\mathcal{L}(x, y, \lambda) = f(x, y) - \lambda(g(x, y) - c)
\]

---

## Linear Algebra

### 3D Surface and Contour Plots

Generated by evaluating functions over a mesh grid and plotting values using matplotlib.

### Eigenvalues/Eigenvectors (Planned)

Will include manual power iteration and QR algorithm for deeper insight into PCA and linear transformations.

---

## Educational Tips

- All formulae are hand-derived and algorithms are coded with minimal library abstraction to aid understanding.
- Visualizations complement theoryâ€”run scripts to see every math concept in action!
